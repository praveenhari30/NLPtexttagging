Data Mining Objectives includes converting flat files data into structured data frame by applying data cleaning techniques and further tagging each customer with the services they utilize over and over. These tagged customers can be used to create classified groups using classification algorithms based on services they used the most. This will help us in detecting the services associated with customers which can be helpful in generating business insights specifically email marketing and top brands that crosses each other. Also, it will help us in increasing customer base for other services and lastly predicting the next services for existing customers be used for marketing purpose.  
Data mining success criteria is to first create a standardized data from the given flat file and then employ a machine learning classifier or recommendation system that would effectively predict the service that the customer like to take next based on the customer’s past taken services. There are several challenges along the way to better prepare the data that can be utilized for the model. Data volume and unstructured data are the foremost challenges to be taken care of. 


Data for this analysis is delivered as a flat file and is sourced from an operational-driven database. The database aggregates various POS-captured data, inclusive of services and consumer data, as well as appended household demographics. Data stored to identify services are not standardized. This data is stored as unstructured text describing customers’ prior services

Of all the variables included in the dataset, most important variable to predict the next likely service of a customer is ‘Job Summary’. ‘Job Summary’ column is an unstructured text field which describes what is the issue, what was done to fix the issue and materials procured to fix the issue. The challenge here is ‘Job Summary’ column is not standardized. For example, Refrigerator is referred as Fridge in some transactions, Refrigerator in some other and even short forms as ‘Refrig’ in few transactions. Since this column is manually entered by workers, it is prone to spelling mistakes as well. So, this column needs most cleaning work as this contains information about what service is being taken by the customer and will play a vital role in predicting the next likely service for customer. Couple of other columns, ‘Recency’ – number of days since the customer has taken last service and ‘Frequency’ – how often customer takes the service, can be valuable inputs to the machine learning model.

Given text data is tab limited and it is combination of structured fields like Customer ID, Job ID and unstructured data i.e) Job summary. Challenge here is to convert this semi-structured data in to perfect structured data frame to carry out analysis in Python. Also, number of records is over 4 Million. Data is prepared by importing sample of data, say 1 Million, at a time in Python and file cleaning process using list, file and string operations are employed to convert untidy file to well structured data frame. This Data frame acts as primary source for further exploratory and predictive analysis. Columns are checked for appropriate data types. Null values are replaced with ‘Other’ if its categorical column and with mean or median if its continuous column. If a record contains more than 60% of its value as null, then those records are dropped as it does not really contribute any useful information. Columns and rows of data frame are manipulated as per needs of exploratory analysis. Below are the detailed data preparation steps to make the data model ready. 


Converting given flat file to python data frame structure is not straightforward. File needs to be cleansed to turn into data frame. Unknown encoding, presence of special characters, presence of unstructured data, extra new line and tab characters makes the records in file highly uneven and difficult to import.
Flat file is tab delimited and encoded with unknown encoding technique. The general form of encoding is UTF-8 or Latin-1 for windows system. But importing file in python using either of these encoding still gives special characters that map to different encoding. So, file is opened in binary mode and then decoded, and records are stored in list. Opening file in binary mode solves 2 issues – it avoids special characters and file reading is much faster than opening in a normal read mode.
‘Job Summary’ Column in the data file is an unstructured text column and it has new lines and tabs that varies from record to records making the lines in flat file uneven. For example, consider the below screenshot of 2 records from the file which are highlighted in green and red respectively. Green record has only one line (i.e.) only one-line number) in the file which means that this record can be imported into a data frame record with no issues. But, red highlighted record has multiple lines in the file even though logically they belong to same record. So, this line will be imported as multiple records in python since it has multiple lines in the file and it is undesirable. 


There are multiple records in the flat file with this issue. To overcome this, each line in the flat file is gone through one by one and verified and corrected programmatically. Logic implemented here is each line should start with Customer ID followed by MRC code as per the standard record format. If the line does not start with Customer ID, then it is identified as part of previous line which has customer ID as its start value and is appended to it. In the above example, red highlighted record has 8 lines which belong to single record. First line starts with customer ID followed by MRC code. Hence it will be considered a new record. The next line (“Attempted to clear Obstruction using surer”) does not start with customer ID and is continuation of Job summary. So, this line will be appended to previous line which has started with customer ID. Same principle to applied to other 6 lines of the red highlighted record and lines will be read as a single record in the python List. 
Once the lines are read properly into python lists, extra tabs and new line characters are removed so that each field is separated by one-tab character. Now the data is structured in to data frame from the lists which gives proper rows and column structure to work with as shown below.


All these columns are dropped from the analysis perspective as they do not convey enough information to be considered to include in the model. Including columns with lot of Null values will also make the model more biased. Hence these columns are dropped. Job summary column has around 43.5% of null values. Since this column is most important in tagging a service to the customer transaction and it cannot be replaced with statistical techniques, records with Null Job Summary are not considered in the further analysis.


In this phase of the project in which we prepare the data to feed into machine learning models, this is the one of the most important steps before preparing models for predictions. We have used Topic Modeling technique to prepare the dataset which can be used as an input data to Naïve Classifier.  

Topic Modeling is a type of statistical model for discovering the frequent words occurrence in the documents of collections, it is most frequently used Text Mining tool for finding the hidden semantic structures which have repeated most often in huge dataset, by using this technique we can organize large amount of unstructured data into organized way, which can be used to create business insights.
We have widely used topic modeling to identify the type of services mainly used by customers, to process information from our collection of documents which is job summary column (from Master Data Frame), which after data preprocessing contains addresses where services are rendered, amount paid and many more.


In this step of data preparation , we have used Topic Modeling to identify the most used services among all customers bases in each category like Kitchen , Laundry and many more  and as soon as we realized the most frequent services for all the customers within each category  , we have further divided all types of services rendered into categories as per residential and commercial customers under each subsidiary of services offered

After the completion of the above process and we have renamed the services as per understanding of the business and made them suitable analyze for further prediction of services for customers, few examples of such corrections are mentioned below 
•	refrigerator_repairs as Refrigerator Repairs
•	oven as Ovens, Stove Tops & Ranges
•	microwave as Microwave Oven Repairs
•	hydro scrub as Hydro Scrub – Jetting
•	plumbing replacement as Plumbing Replacement & Installations
From this process we have arrived at the topics that needs to be searched for in each Job Summary for each instance of service offered to the customers. To accomplish this first the natural language needs to be processed. Following are the natural language processing techniques that we have used to process the text.
•	Stop word removal
Stop Words: A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.
We would not want these words taking up space in our database or taking up valuable processing time. For this, we can remove them easily, by storing a list of words that you consider to be stop words. NLTK (Natural Language Toolkit) in python has a list of stop words stored in 16 different languages. You can find them in the nltk_data directory.
•	Punctuation removal
Punctuations are redundant information when it comes to topic modelling since the stemmed words within the text will only contribute towards text classification. These can be removed through String package in python which has a list of punctuations stored.
•	Text Standardization
Stemming and Lemmatization are the basic text processing methods for English text. The goal of both stemming and lemmatization is to reduce inflectional forms and sometimes derivationally related forms of a word to a common base form
•	Lemmatization
Lemmatisation (or lemmatization) in linguistics, is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item.
Lemmatisation is closely related to stemming. The difference is that a stemmer operates on a single word without knowledge of the context, and therefore cannot discriminate between words which have different meanings depending on part of speech. However, stemmers are typically easier to implement and run faster, and the reduced accuracy may not matter for some applications.
•	Stemming
Stemming is the process for reducing inflected (or sometimes derived) words to their stem, base or root form—generally a written word form. The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root.

All these steps are done for both the topics (Service types offered) and Job Summaries, thus all texts are brought down to their basic structure and post that the probability of a topic (Service Type) falling in the text (Job summary) is calculated through Naïve Bayes Classification as illustrated in the following section.

Naïve Bayes Classifier belongs to a family of probabilistic classifiers which is purely based on Bayes’ theorem, it assumes that the feature in a class is independent of any other class in the dataset and further it is used to predict the class of an unknown feature in a dataset.


Mechanism behind the classification techniques involves mathematical calculation which are briefly described in 3 steps :
1.	In this step, Naïve Bayes Classifier algorithm converts the dataset into frequency table that means it will create the count of each words occurred in the dataset. In our case, it will create the frequency table for each type of service identified by the topic modeling with categories. Suppose for example let’s say Refrigerators occurs 10000 times, Hydro Scrub occurs like 16000 times and many more services which are identified will have the count too respectively 
2.	As soon as we have frequency table prepared for each kind of services type, this step involves in creating the likelihood table. Likelihood table is the table which contains overcast probabilities for each type of services used by the Residential and Commercial customers 
3.	In this final step, algorithm calculates the posterior probability, which is the probability for each type of service will be used by customer in the future given the customer has used one or many services in the past. This step is highly useful in the calculating the possible posterior probabilities for each type of services and the services which has the highest probability will be selected as the suggested service for the customer.
4.	Furthermore, we have tagged the outcome of the services predicted by Naïve Bayes against the customer, this has helped us the realized the most frequent services used by the customer as per their needs. This dataset is highly helpful to create a Recommendation system to predict the next likely service customer will take, which will be helpful in email marketing campaigns.


Recommendation systems are built to identify the likely services that customers will avail next, which is purely based on the services customer had used in the past. Recommendation systems works based on collaborative filtering and Naïve Bayes Classifiers, they are generally called Collaborative filtering recommendations engines. Mechanism to predict the next likely service for customer uses only 3 pieces of information which are briefly mentioned below 
•	Master Customer ID – Unique ID assigned to each customer for which the services are rendered
•	Services ID – Unique ID for each type of service used by customers  
•	Affinity Score – This would be score between 0 to 1 and if the score for the next likely services will be higher, we will the choose those service and suggest the service for customers. Affinity score can be the built on different scales like scales from 0 to 10, scales from 1 to 100.

To organize data for inputs to recommendation system, we have manipulated and filtered many fields that were the outputs of our Naïve Bayes Classifier. First, we have only selected 2 columns out of total columns namely Services Type Tag and Master ID of the customers whom had used those services. In this step, we have assigned each service under tag called Services ID which are uniquely assigned to each type of services 

